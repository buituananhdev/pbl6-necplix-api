{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmX06c8s60Cx",
    "outputId": "624bd8a7-a7b2-47a6-f207-1f908db3557c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83oiuJCa6nkm",
    "outputId": "44bcfa4a-d226-4606-db1f-5d63e43e7530"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Read dataset\n",
    "c_df=pd.read_csv('/content/drive/MyDrive/PBL6/data_processing/Data/data_1/content_based_filtering/credits_1.csv')\n",
    "m_df=pd.read_csv('/content/drive/MyDrive/PBL6/data_processing/Data/data_1/content_based_filtering/movies_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HIDq5OB6nkp"
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['budget', 'homepage', 'original_language', 'popularity', \n",
    "                   'production_companies', 'production_countries', 'release_date', \n",
    "                   'revenue', 'runtime', 'spoken_languages', 'status']\n",
    "\n",
    "# Check and remove \n",
    "m_df = m_df.drop([col for col in columns_to_drop if col in m_df.columns], axis=1)\n",
    "# Renaming columns in c_df for better readability\n",
    "c_df.columns = ['id','tittle','cast','crew']\n",
    "# Merging the two datasets (c_df and m_df) based on the 'id' column\n",
    "m_df= m_df.merge(c_df,on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HA2w_9nmu2rC"
   },
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters and convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text).lower().strip()\n",
    "\n",
    "    # Split text into words and remove stopwords\n",
    "    words = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize each word in the list\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Return the processed text as a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to convert genres JSON string to readable text\n",
    "def genres_to_text(genres_json):\n",
    "    try:\n",
    "        # Replace single quotes with double quotes if necessary (to ensure valid JSON format)\n",
    "        genres_json = genres_json.replace(\"'\", '\"')\n",
    "        \n",
    "        # Try to parse the JSON string into a list\n",
    "        data = json.loads(genres_json)\n",
    "        \n",
    "        # Extract and return the genre names as a comma-separated string\n",
    "        return \", \".join([genre[\"name\"] for genre in data])\n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON parsing fails, return an empty string or an error message\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 'genres_to_text' function to convert the 'genres' column to a text string\n",
    "m_df['genres'] = m_df['genres'].apply(genres_to_text)\n",
    "\n",
    "# Apply the 'genres_to_text' function to convert the 'keywords' column to a text string\n",
    "m_df['keywords'] = m_df['keywords'].apply(genres_to_text)\n",
    "\n",
    "# Fill missing values in the 'overview' column with an empty string\n",
    "m_df['overview'] = m_df['overview'].fillna('')\n",
    "\n",
    "# Apply text preprocessing to the 'genres' and 'keywords' columns\n",
    "m_df['genres'] = m_df['genres'].apply(preprocess_text)\n",
    "m_df['keywords'] = m_df['keywords'].apply(preprocess_text)\n",
    "\n",
    "# Combine the 'overview', 'genres', and 'keywords' columns into a new 'combined' column\n",
    "m_df['combined'] = m_df['overview'] + ' ' + m_df['genres'] + ' ' + m_df['keywords']\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "m_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b5FrWX46nkq"
   },
   "outputs": [],
   "source": [
    "m_df.to_csv(\"processed_data.csv\", index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
